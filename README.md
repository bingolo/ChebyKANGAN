# ğŸ”¥ ChebyKANGAN: A Polynomially Adaptive GAN for Wildfire Burned Area Segmentation  
### Comprehensive Ablation Study & Literature Comparison

This repository provides a **comprehensive experimental analysis** of **ChebyKANGAN**, a GAN-based architecture developed for **automatic burned area segmentation from multispectral satellite imagery**, enhanced with **Chebyshev Kolmogorovâ€“Arnold Network (ChebyKAN)** integration.

This repo is prepared to support the following paper:

> **â€œChebyKANGAN: A Polynomially Adaptive GAN for Wildfire Burned Area Segmentationâ€**  
> *(Manuscript under review)*

The study is designed to deliver **quantitative and qualitative comparisons** across architectural choices, loss functions, training strategies, and widely-used baseline methods from the literature.

---

## ğŸ“Œ Objectives

The main objectives of this work are:

- To investigate **where ChebyKAN modules are most effective** within UNet-based segmentation architectures  
- To analyze the impact of different **pixel-level loss functions** (L1, Dice, Focal, etc.) on segmentation performance  
- To compare different optimization and learning strategies in terms of **convergence, stability, and efficiency**  
- To fairly benchmark the proposed best configuration against **commonly used GAN-based methods** in the literature  

---

## ğŸ§  Study Scope

This repository executes the following experimental stages in a **single end-to-end pipeline**:

### 1ï¸âƒ£ Architecture Ablation (ChebyKAN Integration)
ChebyKAN layers are integrated into a UNet-like generator at different locations:

- Encoder
- Decoder
- Encoder + Decoder
- Bottleneck
- Encoder + Bottleneck
- Bottleneck + Decoder  

A total of **6 architectural configurations** are evaluated.

---

### 2ï¸âƒ£ Loss Function Ablation
Using the best architecture, multiple pixel-level loss functions are tested:

- L1 Loss  
- Dice Loss  
- Focal Loss  

---

### 3ï¸âƒ£ Training Strategy Ablation
Using the best architecture + best loss function, multiple training strategies are compared:

- Adam (standard)
- Adam (higher learning rate)
- RMSProp
- SGD + Cosine Annealing  

---

### 4ï¸âƒ£ Literature Comparison
The best proposed model is compared with the following baseline methods:

- Pix2Pix GAN  
- CycleGAN  
- Attention UNet GAN  
- UNet++ GAN  
- WGAN (Spectral Normalization)  

---

## ğŸ“¦ Dataset & Data Release Policy

This repository provides a **small sample dataset** for reproducibility and quick testing.

### âœ… Sample dataset included in this repo (GitHub)
Only **27 image-mask pairs** are included in this repository:

- `data/sample/images/` â†’ 27 multispectral images  
- `data/sample/masks/`  â†’ 27 corresponding segmentation masks  

This sample is provided to:
- validate that the pipeline runs correctly,
- demonstrate the required folder structure,
- enable fast demo/testing scenarios.

### ğŸŒ Full dataset (Kaggle)
Due to GitHub size limitations, the full dataset is **not** included in this repository.  
The complete dataset will be published via Kaggle:

- Kaggle Dataset Link: **https://www.kaggle.com/datasets/bingolo/wildfire-burned-area-segmentation-dataset/data**

After downloading the full dataset, it should follow this structure:

```text
data/
  images/
  masks/
```

Then, you only need to update the dataset paths in the configuration file.

---

## ğŸ“‚ Project Structure

```text
ChebyKANGAN/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample/
â”‚   â”‚   â”œâ”€â”€ images/               # 27 sample multispectral images
â”‚   â”‚   â””â”€â”€ masks/                # 27 corresponding masks
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml              # Paths + experiment hyperparameters
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py                 # Config loader
â”‚   â”œâ”€â”€ seed.py                   # Reproducibility (seed control)
â”‚   â”œâ”€â”€ io_utils.py               # JSON / file utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py            # WildfireDataset class
â”‚   â”‚   â””â”€â”€ splits.py             # Train/test split + NaN checks
â”‚   â”‚
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â””â”€â”€ segmentation_losses.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ blocks.py             # ChebyKAN, Attention, Residual blocks
â”‚   â”‚   â”œâ”€â”€ generators.py         # UNet, UNet++, Pix2Pix, CycleGAN, etc.
â”‚   â”‚   â””â”€â”€ discriminators.py
â”‚   â”‚
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ gan_trainer.py        # Training loop
â”‚   â”‚   â””â”€â”€ evaluator.py          # Evaluation logic
â”‚   â”‚
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â””â”€â”€ segmentation_metrics.py
â”‚   â”‚
â”‚   â”œâ”€â”€ vis/
â”‚   â”‚   â”œâ”€â”€ plots.py              # Curves + correlation plots
â”‚   â”‚   â””â”€â”€ tables.py             # Excel/CSV export
â”‚   â”‚
â”‚   â””â”€â”€ experiments/
â”‚       â””â”€â”€ ablations.py          # Full experiment pipeline
â”‚
â”œâ”€â”€ run.py                        # Main entry point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ“Š Outputs Produced

For each ablation study and comparison, the pipeline automatically produces:

### ğŸ“ˆ Figures
- Training loss curves (Generator & Discriminator)
- Convergence analysis (combined loss, G/D ratio)
- Performance correlations (F1 vs IoU, Precision vs Recall)
- Metric comparison bar charts and heatmaps
- Qualitative sample predictions (visual outputs)
- Radar charts for top-5 models

### ğŸ“„ Tables
- Excel (`.xlsx`) exports including:
  - Test metrics
  - Model rankings (Overall Rank)
  - Training time statistics
  - Parameter count statistics

### ğŸ§¾ Reports
- `summary_report.json`  
- `COMPREHENSIVE_RESULTS.xlsx`

---

## âš™ï¸ Installation

```bash
git clone <repo_url>
cd ChebyKANGAN

python -m venv .venv
# Windows
.venv\Scripts\activate

pip install -r requirements.txt
```

> **Note:** `rasterio` is required for `.tif/.tiff` files.  
> If you use `.npy` format only, `rasterio` is not mandatory.

---

## ğŸš€ Running Experiments

### 1) Run with the sample dataset (27 pairs)
Update `configs/default.yaml` as follows:

```yaml
DATA_PATH: "data/sample/images"
LABEL_PATH: "data/sample/masks"
```

Then run:

```bash
python run.py
```

---

### 2) Run with the full dataset (Kaggle)
Download the full dataset and place it as:

```text
data/images/
data/masks/
```

Then update the config:

```yaml
DATA_PATH: "data/images"
LABEL_PATH: "data/masks"
```

Run:

```bash
python run.py
```


---

## ğŸ” Reproducibility

This repository is designed with reproducibility in mind:

- fixed random `SEED` usage  
- deterministic train/test split  
- automatic logging of all experiments  
- automated metric reporting and exports  

This setup is suitable for **academic publications**, **thesis work**, and **benchmarking**.

---

## ğŸ“œ License

### Code License
The source code in this repository is released under the **MIT License**.  

### Dataset License
The wildfire burned area segmentation dataset is released under the  
**Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)**.

- Kaggle release: CC BY-NC 4.0  
- GitHub sample data (`data/sample/`): CC BY-NC 4.0  


## ğŸ“š Academic Use & Citation

This work is intended to serve as a reference for research in:
**wildfire segmentation**, **remote sensing**, **deep learning-based segmentation**, and **KAN-based architectures**.

If you use the dataset and codes, please cite the following paper:

### BibTeX (Paper under review)
```bibtex
@article{chebykangan2026,
  title   = {ChebyKANGAN: A Polynomially Adaptive GAN for Wildfire Burned Area Segmentation},
  author  = {Under Review (Double Blind) },
  journal = {Under Review},
  year    = {2026}
}
```

---

## âš ï¸ Notes & Limitations

- The pipeline assumes the multispectral input has a number of bands consistent with `NUM_BANDS`.
- Some datasets store masks in 0/255 format; the code automatically normalizes masks into 0/1 format.
- Training time may significantly vary depending on GPU hardware and dataset size.

---

## ğŸ“¬ Contact

For questions, improvement suggestions, or academic collaboration:

ğŸ“§ **email : Under Review (Double Blind)**
